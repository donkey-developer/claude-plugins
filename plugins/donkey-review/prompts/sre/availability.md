# Availability

Can the service withstand dependency failures and keep serving requests?

The Availability pillar evaluates whether the code's retry logic, timeouts, health checks, and fault isolation mechanisms prevent dependency failures from becoming service failures.
When this pillar is weak, a single failing dependency can cascade into a full service outage.

## Focus Areas

The Availability pillar applies the SEEMS/FaCTOR duality through two specific lenses.

### SEEMS focus (how the code fails)

- **Excessive load** — Unbounded or poorly configured retry logic amplifies load on a failing dependency.
  A service retrying forever under failure generates more traffic than normal operation, making recovery impossible.
- **Excessive latency** — Missing or oversized timeouts allow a hung dependency to block all worker threads.
  When threads are exhausted, the service becomes completely unresponsive to all callers.
- **Single points of failure** — Hard dependencies on caches, circuit breakers without recovery paths, and health checks that do not verify dependencies all create hidden single points of failure.
  When they fail, the service fails with them.
- **Shared fate** — Synchronous calls to non-critical services in the critical request path couple their availability to the core service.
  A slow analytics call should not make a payment fail.

### FaCTOR focus (what should protect against failure)

- **Capacity** — Retry logic must be bounded to prevent retry storms from overwhelming a recovering dependency.
  Load generated by retries is just as dangerous as load from new requests.
- **Timeliness** — Timeouts must be set and sized to match user expectations, not just infrastructure limits.
  A 60-second timeout on a user-facing call offers no protection from a user perspective.
- **Availability** — Health checks must reflect the true readiness of the service to handle requests.
  An inaccurate health check is worse than no health check — it provides false confidence.
- **Redundancy** — Caches and optional dependencies must be treated as performance optimisations, not hard dependencies.
  Fallback paths must exist when they are unavailable.
- **Fault isolation** — Non-critical work must not run in the critical request path.
  Failures in non-critical dependencies must not propagate to critical operations.

## Anti-Pattern Catalogue

### AP-01: Unbounded retry

```python
while True:
    try:
        return dependency.call()
    except Exception:
        time.sleep(1)
```

**Why it matters:** Under dependency failure, every in-flight request retries indefinitely.
Load on the failing dependency increases instead of decreasing.
Recovery becomes impossible because the retry traffic overwhelms the dependency the moment it tries to come back.
SEEMS: Excessive load.
FaCTOR: Capacity (no bound on retry-generated load).
Typical severity: HIGH / HYG (Total -- can take down the service and prevent dependency recovery).

### AP-02: Retry without backoff

Retry with a fixed delay or no delay at all between attempts.

**Why it matters:** All retries fire at the same interval, maintaining constant pressure on the failing dependency.
There is no space for the dependency to recover between retry waves.
SEEMS: Excessive load.
FaCTOR: Capacity.
Typical severity: HIGH / L1.
Escalates to HYG if the retry is also unbounded (AP-01 compound).

### AP-03: Retry without jitter

Exponential backoff but all clients use the same delay schedule (e.g., 1s, 2s, 4s, 8s with no randomisation).

**Why it matters:** Thundering herd -- all clients that failed at the same time retry at the same time, creating periodic spikes of load on the dependency.
The synchronised retry pattern can repeat indefinitely even with bounded retries.
SEEMS: Excessive load.
FaCTOR: Capacity.
Typical severity: MEDIUM / L2.

### AP-04: Missing timeout on external call

```python
response = requests.post(url, json=payload)
db.execute(query)
cache.get(key)
```

**Why it matters:** A hung dependency blocks the calling thread indefinitely.
Under failure, all worker threads can be consumed waiting on the hung dependency, making the service completely unresponsive to all callers.
SEEMS: Excessive latency.
FaCTOR: Timeliness.
Typical severity: HIGH / HYG (Total -- if in the request path, can render the service entirely unresponsive).
Reduces to MEDIUM / L1 if the call is in a background worker pool isolated from the request-handling path.

### AP-05: Timeout longer than user patience

A 30-second or 60-second timeout on a call that is part of a user-facing request where users expect a response in 3-5 seconds.

**Why it matters:** The timeout "protects" the service but the user has already given up and retried (or left).
Resources are tied up serving a response nobody will receive, and retry attempts from impatient users amplify load further.
SEEMS: Excessive latency.
FaCTOR: Timeliness.
Typical severity: MEDIUM / L2.

### AP-06: Health check doesn't check dependencies

```python
@app.route("/health")
def health():
    return {"status": "healthy"}, 200
```

**Why it matters:** The service reports healthy but cannot serve requests when its dependencies are down.
Load balancers and orchestrators route traffic to it, and users receive errors from an instance that claims to be ready.
SEEMS: Misconfiguration.
FaCTOR: Availability.
Typical severity: MEDIUM / L1.
Escalates to HIGH / HYG if the health check always returns healthy regardless of any internal state (Total -- routes all traffic into a broken instance).

### AP-07: Health check too sensitive (flapping)

Health check fails on a single transient error without any dampening or consecutive-failure threshold.

**Why it matters:** Transient errors cause the health check to flap between healthy and unhealthy.
The orchestrator or load balancer repeatedly adds and removes the instance, creating instability that compounds the original transient problem.
SEEMS: Single points of failure (availability depends on a single probe succeeding).
FaCTOR: Availability.
Typical severity: MEDIUM / L2.

### AP-08: Synchronous call to non-critical service

A user-facing request handler makes a synchronous, inline call to a non-critical service (analytics, recommendations, audit logging) in the request path.

**Why it matters:** If the non-critical service is slow or down, the critical user-facing request is degraded or fails.
Non-critical work should never block the critical path -- it should be offloaded to a queue, a background goroutine, or a fire-and-forget call with its own timeout.
SEEMS: Shared fate (non-critical dependency failure cascades to the critical path).
FaCTOR: Fault isolation.
Typical severity: MEDIUM / L2.

### AP-09: No fallback when cache is unavailable

Cache miss or cache failure causes the request to fail entirely, rather than falling back to the source of truth (database, API).

**Why it matters:** The cache was introduced to improve performance, but it has become a hard dependency.
Cache failure equals service failure, even though a slower path via the source of truth is available.
SEEMS: Single points of failure (cache is a SPOF).
FaCTOR: Redundancy.
Typical severity: MEDIUM / L2.

### AP-10: Circuit breaker that never closes

Circuit breaker opens when a dependency fails but has no half-open state or recovery probe.
Once open, it stays open permanently.

**Why it matters:** The circuit breaker correctly protects the service during failure but never allows recovery.
The dependency recovers but the service never notices, resulting in permanent degradation that requires a manual intervention or restart to resolve.
SEEMS: Single points of failure (recovery path is broken).
FaCTOR: Availability.
Typical severity: MEDIUM / L2.

## Review Checklist

When assessing the Availability pillar, work through each item in order.

1. **Retry bounds** -- Do all retry loops have a maximum attempt count? Are there any `while True` retry patterns or retry loops with no exit condition?
2. **Retry backoff** -- Does retry logic use exponential backoff? Are retries with fixed delays or no delays flagged?
3. **Retry jitter** -- Does exponential backoff include randomised jitter to prevent thundering herd? Are all clients using identical delay schedules?
4. **Timeout coverage** -- Do all external calls (HTTP, database, cache, queue, gRPC) have explicit timeouts configured? Are there calls with no timeout parameter?
5. **Timeout sizing** -- Are timeouts sized relative to user-facing latency expectations, not just infrastructure limits? Are 30-second or 60-second timeouts present on user-facing request paths?
6. **Health check accuracy** -- Do health checks verify critical dependencies (database, cache, queue) before returning healthy? Are health checks that always return 200 flagged immediately?
7. **Health check stability** -- Do health checks use consecutive-failure thresholds or dampening to prevent flapping on transient errors?
8. **Critical path isolation** -- Do user-facing request handlers avoid synchronous calls to non-critical services? Are analytics, audit, and notification calls offloaded from the critical path?
9. **Cache fallback** -- When cache operations fail, does the code fall back to the source of truth rather than failing the request?
10. **Circuit breaker recovery** -- Do circuit breakers implement a half-open state with a recovery probe? Can they close automatically when the dependency recovers?

## Severity Framing

Severity for Availability findings is about blast radius -- what happens to the service when this code path is hit under dependency failure.

- **Total failure risk** -- Missing timeouts in the request path and unbounded retries can consume all worker threads or overwhelm a recovering dependency.
  These are Hygiene findings because they can render the entire service unresponsive.
- **Amplified load** -- Retry without backoff or jitter maintains or increases load on a failing dependency.
  The retry behaviour that is meant to help can become the cause of extended outages.
- **Silent degradation** -- Health checks that do not verify dependencies and circuit breakers that never close create states where the service is technically running but cannot serve requests.
  These are hard to detect because the service process appears healthy.
